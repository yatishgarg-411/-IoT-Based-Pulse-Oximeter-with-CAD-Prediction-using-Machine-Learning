{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'./dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Summary and Info  \n",
    "- **`df.describe()`**: Provides statistical insights like mean, min, max, and percentiles for numerical columns.  \n",
    "- **`df.info()`**: Displays the DataFrame's structure, data types, and non-null value counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since Sex attribute is a categorical attribute, we will convert it to a numerical attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding male as '1' and Female as'0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'Male' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution\n",
    "\n",
    "- **`df['Status'].value_counts()`**: Displays the number of rows (frequency) for each unique class in the `Status` column.  \n",
    "  This is useful for understanding the distribution of data across different categories and checking for class imbalance in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows of each class\n",
    "df['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Understanding the Data Through Plots\n",
    "\n",
    "Here’s what we did to explore the data and understand how different features relate to the `Status` class:  \n",
    "\n",
    "1. **Age Distribution**  \n",
    "   - We plotted the `Age` values for each class in the `Status` column.  \n",
    "   - This helps us see if certain age groups are more common in each class.  \n",
    "\n",
    "2. **BMI Distribution**  \n",
    "   - This shows how `BMI` (Body Mass Index) is spread out for each class.  \n",
    "   - It helps us check if BMI has any role in differentiating between the classes.  \n",
    "\n",
    "3. **Temperature Distribution**  \n",
    "   - We looked at how `Temperature` changes for each class.  \n",
    "   - This can tell us if people in different classes have noticeable differences in body temperature.  \n",
    "\n",
    "4. **Heart Rate Distribution**  \n",
    "   - This plot shows how `Heart_rate` values are distributed for each class.  \n",
    "   - It helps us understand if heart rate is an important factor.  \n",
    "\n",
    "5. **SPO2 Distribution**  \n",
    "   - We explored the `SPO2` (oxygen levels) to see if it varies across classes.  \n",
    "   - It’s useful for checking if oxygen levels are related to the classes.  \n",
    "\n",
    "6. **ECG Distribution**  \n",
    "   - Finally, we looked at the `Ecg` (electrocardiogram) values for each class.  \n",
    "   - This can help us see if ECG readings differ between the groups.  \n",
    "\n",
    "By plotting these features, we get a clearer idea of how the data behaves and what might be important for analysis!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda\n",
    "\n",
    "# Plot the distribution of 'Age' for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='Age', hue='Status', kde=True, bins=30)\n",
    "plt.title(\"Distribution of Age for each CAD Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of 'BMI' for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='BMI', hue='Status', kde=True, bins=30)\n",
    "plt.title(\"Distribution of BMI for each CAD Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of 'Temperature' for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='Temperature', hue='Status', kde=True, bins=30)\n",
    "plt.title(\"Distribution of Temperature for each CAD Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of 'Heart_rate' for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='Heart_rate', hue='Status', kde=True, bins=30)\n",
    "plt.title(\"Distribution of Heart Rate for each CAD Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of 'SPO2' for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='SPO2', hue='Status', kde=True, bins=30)\n",
    "plt.title(\"Distribution of SPO2 for each CAD Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of 'Ecg' for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='Ecg', hue='Status', kde=True, bins=30)\n",
    "plt.title(\"Distribution of ECG for each CAD Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix  \n",
    "\n",
    "- A **correlation matrix** shows how strongly different features in the dataset are related to each other.  \n",
    "- We used a heatmap to visualize the correlations, where:  \n",
    "  - Values range from **-1 to 1**:\n",
    "    - **1** means perfect positive correlation (features increase together).  \n",
    "    - **-1** means perfect negative correlation (one feature decreases as the other increases).  \n",
    "    - **0** means no correlation.  \n",
    "  - The colors help make it easy to spot strong positive (red) or negative (blue) relationships.  \n",
    "\n",
    "- This plot is useful to identify which features are strongly linked to each other or the target variable (`Status`), guiding us in feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Low-Correlation Features  \n",
    "\n",
    "- We removed the following columns: **`Age`**, **`Weight`**, **`Height`**, **`Sex`**, and **`BMI`**.  \n",
    "- **Reason**:  \n",
    "  - These features showed **low correlation** with the target variable (`Status`) in the correlation matrix.  \n",
    "  - Features with low or no correlation often do not contribute significantly to the model's performance.  \n",
    "  - Keeping them can add unnecessary noise and increase computation time.  \n",
    "\n",
    "- By dropping these features, we focus only on the most relevant data for better analysis and model accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop low correlation features\n",
    "df = df.drop(columns=['Age', 'Weight', 'Height', 'Sex', 'BMI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated Correlation Matrix  \n",
    "\n",
    "- After dropping low-correlation features, we created a new **correlation matrix** to reassess the relationships between the remaining features.  \n",
    "- **Why check again?**  \n",
    "  - To ensure that the dropped features did not affect the strength of the remaining correlations.  \n",
    "  - This helps confirm that the dataset is now more focused on relevant features that might impact the target variable (`Status`).  \n",
    "\n",
    "- The updated heatmap highlights stronger and cleaner relationships, making it easier to identify the key features for further analysis or modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Training and Testing Sets  \n",
    "\n",
    "- **`X`**: This is the feature set, where we removed the target variable (`Status`) because it is what we want to predict.  \n",
    "- **`y`**: This is the target variable (`Status`), which we want to predict using the features.  \n",
    "\n",
    "- We used the **`train_test_split()`** function to split the data:  \n",
    "  - **Training Set**: 80% of the data used to train the model.  \n",
    "  - **Testing Set**: 20% of the data set aside to evaluate the model's performance.  \n",
    "  - **`random_state=42`**: Ensures that the split is reproducible, so you get the same training and test sets each time.  \n",
    "\n",
    "This split is crucial for training and validating the model, helping us assess how well it will perform on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "X = df.drop(columns=['Status'])\n",
    "y = df['Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset is tested on different models and Accuracy is calculated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression  \n",
    "\n",
    "1. **What is Logistic Regression?**  \n",
    "   Logistic Regression is a classification model that predicts the probability of a binary outcome (e.g., presence or absence of CAD). It uses the logistic (sigmoid) function to map predictions to probabilities between 0 and 1.  \n",
    "\n",
    "2. **Steps in the Code**:  \n",
    "   - **Model Initialization**: The Logistic Regression model is created with `random_state=42` for consistent results and `max_iter=100` to control convergence.  \n",
    "   - **Training**: The model is trained on the `X_train` (features) and `y_train` (labels).  \n",
    "   - **Accuracy Calculation**: The model's performance is evaluated using `score()` on the test dataset, showing the percentage of correct predictions.  \n",
    "\n",
    "3. **Output**:  \n",
    "   The model's accuracy is printed, representing how well it predicts CAD (Coronary Artery Disease) based on the input features.  \n",
    "\n",
    "**Logistic Regression** is a straightforward and reliable algorithm, especially for linearly separable datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=100)\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg_score = log_reg.score(X_test, y_test)\n",
    "print(f\"Logistic Regression Model Accuracy: {log_reg_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)  \n",
    "\n",
    "1. **What is SVM?**  \n",
    "   SVM is a supervised learning algorithm used for classification tasks. It works by finding the best hyperplane that separates data points of different classes.  \n",
    "   - The **RBF Kernel** (Radial Basis Function) allows SVM to handle non-linear relationships by transforming data into higher dimensions.  \n",
    "\n",
    "2. **Steps in the Code**:  \n",
    "   - **Model Initialization**: The SVM model is set up with an **RBF kernel**, `random_state=42` for reproducibility, and `probability=True` to enable prediction probabilities (useful for ensemble methods).  \n",
    "   - **Training**: The model is trained on `X_train` (features) and `y_train` (labels).  \n",
    "   - **Accuracy Calculation**: The model is tested on `X_test` and `y_test` to calculate its accuracy (percentage of correct predictions).  \n",
    "\n",
    "3. **Output**:  \n",
    "   The accuracy of the SVM model is printed, indicating its performance in predicting CAD (Coronary Artery Disease).  \n",
    "\n",
    "**SVM with an RBF kernel** is powerful for complex datasets with non-linear decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "from sklearn import svm\n",
    "svm_model = svm.SVC(kernel='rbf', random_state=42, probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_score = svm_model.score(X_test, y_test)\n",
    "print(f\"SVM Model Accuracy: {svm_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network (ANN)  \n",
    "\n",
    "1. **What is ANN?**  \n",
    "   - An Artificial Neural Network is a machine learning model inspired by the structure of the human brain.  \n",
    "   - It consists of layers of interconnected nodes (neurons) that process data and extract patterns, making it highly suitable for complex and non-linear relationships in data.\n",
    "\n",
    "2. **Steps in the Code**:  \n",
    "   - **Model Definition**:  \n",
    "     - `Sequential`: The ANN is defined as a stack of layers.  \n",
    "     - **Layers**:  \n",
    "       - `Dense(256, activation='relu', input_shape=(4,))`: First layer with 256 neurons, using ReLU activation, and expecting 4 input features.  \n",
    "       - Additional hidden layers with 128 and 32 neurons, also using ReLU activation.  \n",
    "       - `Dense(1, activation='sigmoid')`: Output layer with a sigmoid activation for binary classification.  \n",
    "   - **Compilation**:  \n",
    "     - **Optimizer**: `adam`, a popular optimizer that adjusts weights efficiently.  \n",
    "     - **Loss Function**: `binary_crossentropy`, appropriate for binary classification tasks.  \n",
    "     - **Metrics**: Tracks accuracy during training.  \n",
    "   - **Training**:  \n",
    "     - `fit()`: The model is trained for 50 epochs with a batch size of 32 and validation using the test set.  \n",
    "   - **Evaluation**:  \n",
    "     - The model is evaluated on the test set, providing the accuracy score.  \n",
    "\n",
    "3. **Output**:  \n",
    "   - The ANN accuracy score indicates how well the model predicts CAD (Coronary Artery Disease) based on the features.\n",
    "\n",
    "**ANNs** are particularly effective for datasets with intricate patterns and relationships that simpler models may not capture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(4,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "_, ann_score = model.evaluate(X_test, y_test)\n",
    "print(f\"ANN Model Accuracy: {ann_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Ensemble Model  \n",
    "\n",
    "1. **Combining Predictions**:  \n",
    "   - Predictions from Logistic Regression, SVM, and ANN are treated as features for the ensemble.  \n",
    "   - `np.column_stack()` combines these predictions into a single feature matrix for the meta-model.  \n",
    "\n",
    "2. **Testing the Ensemble**:  \n",
    "   - The ensemble's accuracy is calculated using the `accuracy_score` function, based on the final predictions from the meta-model.  \n",
    "\n",
    "This approach leverages the strengths of individual models to improve overall prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate predictions for the validation dataset from each model\n",
    "pred_log = log_reg.predict_proba(X_test)[:, 1]  # Logistic Regression probabilities\n",
    "pred_svm = svm_model.predict_proba(X_test)[:, 1]  # SVM probabilities\n",
    "pred_ann = model.predict(X_test).flatten()  # ANN probabilities (ensure 1D array)\n",
    "\n",
    "# Combine predictions as features for the meta-model\n",
    "stacked_features = np.column_stack((pred_log, pred_svm, pred_ann))\n",
    "\n",
    "# Train the meta-model (logistic regression in this case)\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_features, y_test)  # Use test labels to train the meta-model\n",
    "\n",
    "# Test the ensemble on the same test set\n",
    "test_log = log_reg.predict_proba(X_test)[:, 1]\n",
    "test_svm = svm_model.predict_proba(X_test)[:, 1]\n",
    "test_ann = model.predict(X_test).flatten()\n",
    "test_features = np.column_stack((test_log, test_svm, test_ann))\n",
    "\n",
    "# Final predictions from the meta-model\n",
    "final_preds = meta_model.predict(test_features)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "ensemble_accuracy = accuracy_score(y_test, final_preds)\n",
    "print(f\"Stacked Ensemble Accuracy: {ensemble_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting New User Inputs  \n",
    "\n",
    "1. **Process**:  \n",
    "   - Each user input is passed through base models (Logistic Regression, SVM, ANN) to get probabilities.  \n",
    "   - These probabilities are combined into a single feature vector.  \n",
    "   - The final model predicts the class and probability based on the combined features.  \n",
    "\n",
    "2. **Class Labels**:  \n",
    "   - **Class 0**: Refers to **No CAD (Coronary Artery Disease)**.  \n",
    "   - **Class 1**: Refers to **CAD (Coronary Artery Disease)**.  \n",
    "\n",
    "3. **Output**:  \n",
    "   - For each input, the final **class label** (0 or 1) and **probability** of having CAD are displayed.  \n",
    "\n",
    "This method combines predictions from multiple models to improve accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user input\n",
    "user_input = np.array([[28,60,95,0]])  # Replace with actual user input class 1\n",
    "user_input1 = np.array([[32,71,100,0]])  # Replace with actual user input class 0\n",
    "# Preprocess user input (if required, e.g., scaling, normalization, etc.)\n",
    "\n",
    "# Step 1: Get predictions from the base models\n",
    "log_reg_pred = log_reg.predict_proba(user_input)[:, 1]  # Logistic Regression probability\n",
    "svm_pred = svm_model.predict_proba(user_input)[:, 1]  # SVM probability\n",
    "ann_pred = model.predict(user_input).flatten()  # ANN probability\n",
    "\n",
    "# Step 2: Combine the predictions into a single feature vector\n",
    "stacked_input = np.column_stack((log_reg_pred, svm_pred, ann_pred))\n",
    "\n",
    "# Step 3: Pass the feature vector to the meta-model for the final prediction\n",
    "final_prediction = meta_model.predict(stacked_input)  # Class label (e.g., 0 or 1)\n",
    "final_probability = meta_model.predict_proba(stacked_input)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Output the result\n",
    "print(\"First User Input (Class 1):\",user_input)\n",
    "print(f\"Final Prediction (Class): {final_prediction[0]}\")\n",
    "print(f\"Final Prediction (Probability of Class 1): {final_probability[0]:.2f}\")\n",
    "\n",
    "# Step 1: Get predictions from the base models\n",
    "log_reg_pred = log_reg.predict_proba(user_input1)[:, 1]  # Logistic Regression probability\n",
    "svm_pred = svm_model.predict_proba(user_input1)[:, 1]  # SVM probability\n",
    "ann_pred = model.predict(user_input1).flatten()  # ANN probability\n",
    "\n",
    "# Step 2: Combine the predictions into a single feature vector\n",
    "stacked_input = np.column_stack((log_reg_pred, svm_pred, ann_pred))\n",
    "\n",
    "# Step 3: Pass the feature vector to the meta-model for the final prediction\n",
    "final_prediction = meta_model.predict(stacked_input)  # Class label (e.g., 0 or 1)\n",
    "final_probability = meta_model.predict_proba(stacked_input)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Output the result\n",
    "print(\"Second User Input (Class 0):\",user_input1)\n",
    "print(f\"Final Prediction (Class): {final_prediction[0]}\")\n",
    "print(f\"Final Prediction (Probability of Class 1): {final_probability[0]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6082970,
     "sourceId": 9902139,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
